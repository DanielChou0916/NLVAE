{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aeae3d9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-29T17:56:17.922205Z",
     "iopub.status.busy": "2023-06-29T17:56:17.921883Z",
     "iopub.status.idle": "2023-06-29T17:56:21.814096Z",
     "shell.execute_reply": "2023-06-29T17:56:21.812849Z"
    },
    "papermill": {
     "duration": 3.898573,
     "end_time": "2023-06-29T17:56:21.816655",
     "exception": false,
     "start_time": "2023-06-29T17:56:17.918082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from custom_data_loading import *\n",
    "from density_estimation import *\n",
    "from nonlinear_vae import *\n",
    "from distance_correlation import *\n",
    "from spearman_correlation import *\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def stable_cor(z,eps=1e-6):\n",
    "\n",
    "    cov=z.T.cov()\n",
    "    normalized_factor=(cov.diag().reshape(-1,1)*cov.diag().reshape(1,-1)).sqrt()+eps\n",
    "    #normalized_factor=torch.clamp(normalized_factor, min=eps)\n",
    "    R=cov/normalized_factor\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "def elbo_loss_SC(X,X_hat,mean,std,z,beta=0,l1=0,l2=0,l3=50):\n",
    "    #I. the MSE reconstructed loss\n",
    "    recon_loss=(nn.MSELoss(reduction='sum')(X,X_hat))/len(X)\n",
    "    #II. the KLD for posterior\n",
    "    KLD = (0.5 * torch.sum(-1 - (std**2).log() + mean**2 + std**2))/len(X)\n",
    "    #III. Covariance regularization\n",
    "    #Covariance matrix and diag-decomposition\n",
    "    cor_z=z.T.cov()\n",
    "    #cor_z=stable_cor(z)\n",
    "    _,off_diagz=diagonal_decomposition(cor_z)\n",
    "    #Regularization term 1 off_diag\n",
    "    reg=off_diagz.abs().sum()\n",
    "\n",
    "    #Reg 2: spearman_correlation\n",
    "    SC=spearman_correlation_vectorized(z)\n",
    "    _,off_diagSC=diagonal_decomposition(SC)\n",
    "    reg2=off_diagSC.abs().sum()\n",
    "    #reg2=reg2/(len(X)**2-len(X))\n",
    "    total_loss=recon_loss+beta*KLD+l2*reg+l3*reg2\n",
    "    return total_loss,recon_loss,KLD,reg,reg2\n",
    "\n",
    "def elbo_loss_DC(X,X_hat,mean,std,z,beta=0,l1=0,l2=0,l3=50):\n",
    "    #I. the MSE reconstructed loss\n",
    "    recon_loss=(nn.MSELoss(reduction='sum')(X,X_hat))/len(X)\n",
    "    #II. the KLD for posterior\n",
    "    KLD = (0.5 * torch.sum(-1 - (std**2).log() + mean**2 + std**2))/len(X)\n",
    "    #III. Covariance regularization\n",
    "    #Covariance matrix and diag-decomposition\n",
    "    cor_z=z.T.cov()\n",
    "    #cor_z=stable_cor(z)\n",
    "    _,off_diagz=diagonal_decomposition(cor_z)\n",
    "    #Regularization term 1 off_diag\n",
    "    reg=off_diagz.abs().sum()\n",
    "\n",
    "    #Reg 2: Dsitance correlation\n",
    "    DC,_=Distance_CorrCorv_vectorized(z,correlation=True)\n",
    "    _,off_diagDC=diagonal_decomposition(DC)\n",
    "    reg2=off_diagDC.abs().sum()\n",
    "    #reg2=reg2/(len(X)**2-len(X))\n",
    "    total_loss=recon_loss+beta*KLD+l2*reg+l3*reg2\n",
    "    return total_loss,recon_loss,KLD,reg,reg2\n",
    "\n",
    "def elbo_loss_TC(X,X_hat,mean,std,z,beta=0,l1=0,l2=5,l3=50):\n",
    "    #I. the MSE reconstructed loss\n",
    "    recon_loss=(nn.MSELoss(reduction='sum')(X,X_hat))/len(X)\n",
    "    #II. the KLD for posterior\n",
    "    KLD = (0.5 * torch.sum(-1 - (std**2).log() + mean**2 + std**2))/len(X)\n",
    "    #III. Covariance regularization\n",
    "    #Covariance matrix and diag-decomposition\n",
    "    cov_z=z.T.cov()\n",
    "    diagz,off_diagz=diagonal_decomposition(cov_z)\n",
    "    #Regularization term 1 off_diag\n",
    "    #reg=(l1*((diagz-1)**2).sum()+0.5*l2*(off_diagz**2).sum())/len(X)\n",
    "    reg=(off_diagz**2).sum()#/(len(X)**2-len(X))\n",
    "    #Reg 2: sum of mutual info\n",
    "    #1 Calculate the marginal densities and kernel values from skew-normal\n",
    "    _,marginal_k=multi_vars_SkewNormal_density_estimate(z,check=False)\n",
    "    #2 Estimate the total joint kernel values for each sample\n",
    "    #joint,_=estimate_total_joint_density(z)\n",
    "    _,joint_k=estimate_total_joint_density_skew_normal_training(z)\n",
    "    #3 Estimate the total correlation\n",
    "    tc_array=total_correlation_estimation(marginal_k,joint_k)\n",
    "    reg2=tc_array.abs().sum()#/len(X)\n",
    "    total_loss=recon_loss+beta*KLD+l2*reg+l3*reg2\n",
    "    return total_loss,recon_loss,KLD,reg,reg2\n",
    "\n",
    "def elbo_loss_MI(X,X_hat,mean,std,z,beta=0,l1=0,l2=5,l3=50):\n",
    "    #I. the MSE reconstructed loss\n",
    "    recon_loss=(nn.MSELoss(reduction='sum')(X,X_hat))/len(X)\n",
    "    #II. the KLD for posterior\n",
    "    KLD = (0.5 * torch.sum(-1 - (std**2).log() + mean**2 + std**2))/len(X)\n",
    "    #III. Covariance regularization\n",
    "    #Covariance matrix and diag-decomposition\n",
    "    cov_z=z.T.cov()\n",
    "    diagz,off_diagz=diagonal_decomposition(cov_z)\n",
    "    #Regularization term 1 off_diag\n",
    "    #reg=(l1*((diagz-1)**2).sum()+0.5*l2*(off_diagz**2).sum())/len(X)\n",
    "    reg=(off_diagz**2).sum()/(len(X)**2-len(X))\n",
    "    #Reg 2: sum of mutual info\n",
    "    _,mi_sum=pair_wise_MI_upper(z)\n",
    "    reg2=mi_sum/(len(X)**2-len(X))\n",
    "    total_loss=recon_loss+beta*KLD+l2*reg+l3*reg2\n",
    "    return total_loss,recon_loss,KLD,reg,reg2\n",
    "\n",
    "def VAE_list_trainer(model,X_tr_list,X_val_list,param):\n",
    "    lr=param['lr']\n",
    "    max_epoch=param['max_epoch']\n",
    "    batch_sz=param['batch_sz']\n",
    "    Type=param['Type']\n",
    "    beta=param['beta']\n",
    "    l1=param['l1']\n",
    "    l2=param['l2']\n",
    "    l3=param['l3']\n",
    "    by_channel=param['by_channel']\n",
    "    channel_idx=param['channel_idx']\n",
    "    normalize=param['normalize']\n",
    "    loss=param['loss_fn']\n",
    "    optimizor=param['optimizor']\n",
    "    print('The adopted loss:',loss)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        assert (len(X_tr_list)>batch_sz) & (len(X_val_list)>batch_sz)\n",
    "    except:\n",
    "        batch_sz=len(X_val_list)\n",
    "    costs,BCE,KLD,Reg,Reg2=[[],[],[],[],[]]\n",
    "    costs_v,BCE_v,KLD_v,Reg_v,Reg2_v=[[],[],[],[],[]]\n",
    "    \n",
    "    #optimizor=optim.RMSprop(self.parameters(),lr=lr,alpha=0.9)\n",
    "    batch_num_tr=len(X_tr_list)//batch_sz\n",
    "    batch_num_val=len(X_val_list)//batch_sz\n",
    "    for epoch in tqdm(range(max_epoch),desc='Epoch',position=0,leave=True):\n",
    "        #Shffule the lists\n",
    "        random.shuffle(X_tr_list)\n",
    "        random.shuffle(X_val_list)\n",
    "        model.train()\n",
    "        for b in tqdm(range(batch_num_tr),desc=\"Iteration in Epoch:{0}\".format(epoch+1),position=0,leave=False):\n",
    "            #Load batch tensors at once when training!\n",
    "            X_b_list=X_tr_list[b*batch_sz:(b+1)*batch_sz]\n",
    "            X_b=load_a_batch_tensor(X_b_list,by_channel,channel_idx).to(device)\n",
    "            #Z-score standardization\n",
    "            if normalize:\n",
    "                X_b,_,_=batch_tensor_standardization(X_b)\n",
    "                #print('Train normalization is processed!')\n",
    "            ###\n",
    "            mean,std,_,z=model.forward_encoder(X_b)\n",
    "            #try:\n",
    "            #    assert not z.any().isnan()\n",
    "            #except:\n",
    "            #    print(\"Nan value happen in the Encoder\")\n",
    "            #    break\n",
    "            X_hat=model.forward_decoder(z)\n",
    "            #try:\n",
    "            #    assert not X_hat.any().isnan()\n",
    "            #except:\n",
    "            #    print(\"Nan value happen in the Decoder\")\n",
    "            #    break\n",
    "            #mean,log_var,X_hat=model.forward(X_b)\n",
    "            cost,bce,kld,reg,reg2=loss(X_b,X_hat,mean,std,z,beta,l1,l2,l3)\n",
    "            ###\n",
    "            optimizor.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizor.step()\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for v in range(batch_num_val):\n",
    "                X_bv_list=X_val_list[v*batch_sz:(v+1)*batch_sz]\n",
    "                X_bv=load_a_batch_tensor(X_bv_list,by_channel,channel_idx).to(device)\n",
    "                if normalize:\n",
    "                    #Z-score standardization\n",
    "                    X_bv,_,_=batch_tensor_standardization(X_bv)\n",
    "                    #print('valid normalization is processed!')\n",
    "                #mean,log_var,X_hat_val=model.forward(X_bv)\n",
    "                #cost_v,bce_v,kld_v=elbo_loss(X_hat_val,X_bv,mean,log_var,beta)\n",
    "                ###\n",
    "                meanv,stdv,_,zv=model.forward_encoder(X_bv)\n",
    "                X_hat_v=model.forward_decoder(zv)\n",
    "                #mean,log_var,X_hat=model.forward(X_b)\n",
    "                cost_v,bce_v,kld_v,reg_v,reg2_v=loss(X_bv,X_hat_v,meanv,stdv,zv,beta,l1,l2,l3)\n",
    "                ###\n",
    "        \n",
    "        costs.append((cost.detach()).to('cpu'))\n",
    "        BCE.append((bce.detach()).to('cpu'))\n",
    "        KLD.append((kld.detach()).to('cpu'))\n",
    "        Reg.append((reg.detach()).to('cpu'))\n",
    "        Reg2.append((reg2.detach()).to('cpu'))\n",
    "        print('Epoch loss:{0},MSE:{1},Reg1:{2},Reg2:{3}'.format(cost.detach().item(),bce.detach().item(),reg.detach().item(),reg2.detach().item()))\n",
    "        costs_v.append((cost_v.detach()).to('cpu'))\n",
    "        BCE_v.append((bce_v.detach()).to('cpu'))\n",
    "        KLD_v.append((kld_v.detach()).to('cpu'))\n",
    "        Reg_v.append((reg_v.detach()).to('cpu'))\n",
    "        Reg2_v.append((reg2_v.detach()).to('cpu'))\n",
    "\n",
    "    model.cost_dict={'ELBO_tr':costs,'BCE_tr':BCE,'KLD_tr':KLD,'Reg_tr':Reg,'Reg2_tr':Reg2,\\\n",
    "                'ELBO_v':costs_v,'BCE_v':BCE_v,'KLD_v':KLD_v,'Reg_v':Reg_v,'Reg2_v':Reg2_v,}      \n",
    "    return model\n",
    "\n",
    "def plot_loss_reg_VAE(model,mode='Training_only'):\n",
    "    if mode=='Training_only':\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(1,5,1)\n",
    "        plt.plot([c for c in model.cost_dict['ELBO_tr']],label='Train')\n",
    "        plt.title('ELBO')\n",
    "        plt.subplot(1,5,2)\n",
    "        plt.plot([b for b in model.cost_dict['BCE_tr']],label='Train')\n",
    "        plt.title('Reconstruction loss')\n",
    "        plt.subplot(1,5,3)\n",
    "        plt.plot([k for k in model.cost_dict['KLD_tr']],label='Train')\n",
    "        plt.title('KL divergence')\n",
    "        plt.subplot(1,5,4)\n",
    "        plt.plot([k for k in model.cost_dict['Reg_tr']],label='Train')\n",
    "        plt.title('Regularization 1')\n",
    "        plt.subplot(1,5,5)\n",
    "        plt.plot([k for k in model.cost_dict['Reg2_tr']],label='Train')\n",
    "        plt.title('Regularization 2')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(1,5,1)\n",
    "        plt.plot([c for c in model.cost_dict['ELBO_tr']],label='Train')\n",
    "        plt.plot([c for c in model.cost_dict['ELBO_v']],label='Validation')\n",
    "        plt.legend()\n",
    "        plt.title('ELBO')\n",
    "        plt.subplot(1,5,2)\n",
    "        plt.plot([b for b in model.cost_dict['BCE_tr']],label='Train')\n",
    "        plt.plot([b for b in model.cost_dict['BCE_v']],label='Validation')\n",
    "        plt.legend()\n",
    "        plt.title('Reconstruction loss')\n",
    "        plt.subplot(1,5,3)\n",
    "        plt.plot([k for k in model.cost_dict['KLD_tr']],label='Train')\n",
    "        plt.plot([k for k in model.cost_dict['KLD_v']],label='Validation')\n",
    "        plt.legend()\n",
    "        plt.title('KL divergence')\n",
    "        plt.subplot(1,5,4)\n",
    "        plt.plot([k for k in model.cost_dict['Reg_tr']],label='Train')\n",
    "        plt.plot([k for k in model.cost_dict['Reg_v']],label='Validation')\n",
    "        plt.legend()\n",
    "        plt.title('Regularization 1')\n",
    "        \n",
    "        plt.subplot(1,5,5)\n",
    "        plt.plot([k for k in model.cost_dict['Reg2_tr']],label='Train')\n",
    "        plt.plot([k for k in model.cost_dict['Reg2_v']],label='Validation')\n",
    "        plt.legend()\n",
    "        plt.title('Regularization 2')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.714668,
   "end_time": "2023-06-29T17:56:23.141469",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-29T17:56:08.426801",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
